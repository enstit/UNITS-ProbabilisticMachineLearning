{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2.4: Introduction to `PyTorch` ðŸ”¥\n",
    "\n",
    "Probabilistic Machine Learning -- Spring 2023, UniTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch](https://pyth.org/) is a Python (and C++) framework for:\n",
    "- Efficient numerical computing, with (support for) strong GPU acceleration;\n",
    "- Automatic algorithmic differentiation (mainly in *reverse mode*, *tape-based*; but more recently also in *forward mode*);\n",
    "- Development of deep neural models (a.k.a. *deep learning*);\n",
    "\n",
    "It is also well integrated with the *scientific Python stack*.\n",
    "\n",
    "The flexibility of PyTorch and its *Pythonic* interfaces make it the most widely adopted framework for research and development, both in academia and industry (especially industrial *R&D*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It all begins with...\n",
    "import torch\n",
    "import torch as th  # (Absolutely not necessary, but a shorthand)\n",
    "\n",
    "import numpy as np  # For comparison\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operation with `Tensor`s\n",
    "\n",
    "The main building block of PyTorch's linear algebra capabilities is the `Tensor` class. A torch `Tensor` is the (loose) equivalent of NumPy's `ndarray` and most of the functionalities are the same as in NumPy. In general, it is always possible to perform the same logical/mathematical operations typical of NumPy on torch `Tensor`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is x:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "This is y:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "x = th.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(f\"This is x:\\n{x}\")\n",
    "print(f\"This is y:\\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2, 3]), (2, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shapes and sizes\n",
    "x.size(), x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, dtype('int64'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (d)types\n",
    "x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of x before: torch.int64\n",
      "dtype of x after: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# (d)types \"casting\"\n",
    "print(\"dtype of x before:\", x.dtype)\n",
    "x = x.float()\n",
    "print(\"dtype of x after:\", x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of x: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Or with more granular control\n",
    "x = x.to(th.float16)\n",
    "print(\"dtype of x:\", x.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can build a tensor through the constructor `th.Tensor`. In this case, since `th.Tensor` is an alias for `th.FloatTensor`, the tensor you create will have type `th.float32`.\n",
    "\n",
    "More info on data types [here](https://pyth.org/docs/stable/tensors.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor slicing works exactly like in NumPy, by means of square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9661, 0.1468],\n",
      "         [0.0440, 0.7315],\n",
      "         [0.3530, 0.6350]],\n",
      "\n",
      "        [[0.4991, 0.8001],\n",
      "         [0.9371, 0.5635],\n",
      "         [0.8814, 0.2387]]])\n"
     ]
    }
   ],
   "source": [
    "x = th.rand(2, 3, 2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7315) \n",
      "\n",
      "tensor([0.7315, 0.6350]) \n",
      "\n",
      "tensor([[[0.9661, 0.1468],\n",
      "         [0.3530, 0.6350]],\n",
      "\n",
      "        [[0.4991, 0.8001],\n",
      "         [0.8814, 0.2387]]])\n"
     ]
    }
   ],
   "source": [
    "print(x[0, 1, 1], \"\\n\")\n",
    "\n",
    "print(x[0, 1:, 1], \"\\n\")\n",
    "\n",
    "print(x[:, ::2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) \n",
      "\n",
      "torch.Size([])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Note: 0-dimensional tensors vs 1-dimensional tensors\n",
    "print(x[0, 1, 1].shape, \"\\n\")\n",
    "\n",
    "print(th.tensor(3.14).shape)\n",
    "\n",
    "print(th.tensor([3.14]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Use of `numel`\n",
    "print(x.numel())\n",
    "\n",
    "print(th.tensor(3.14).numel())\n",
    "\n",
    "print(th.tensor([3.14]).numel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor reshaping\n",
    "\n",
    "Changing the shape of a tensor can be a crucial operation. To have an idea of its application, just think of `RGB` images.\n",
    "These may be represented as $3\\times H\\times W$ tensors, where H and W stand for height and width of the image (in number of pixels). It is often needed to look at an image as a flattened (1D) vector of pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 8])\n",
      "torch.Size([3, 64])\n"
     ]
    }
   ],
   "source": [
    "img = th.stack((th.ones(8, 8), th.zeros(8, 8), th.ones(8, 8) / 2), dim=0)\n",
    "\n",
    "img.reshape(\n",
    "    3, 64\n",
    ")  # note that reshaping is not in place, so this call does not change the actual shape of img\n",
    "\n",
    "print(img.shape)\n",
    "img2 = img.reshape(3, 64)\n",
    "print(img2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img.numpy())    # It errors: `TypeError: Invalid shape (3, 8, 8) for image data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1f8cbb2230>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_img = img.reshape(8, 8, 3)\n",
    "plt.imshow(new_img.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Difference between:\n",
    "- `view`: operations on contiguous memory;\n",
    "- `reshape`: operations on (non-)contiguous memory, using `view` wherever possible;\n",
    "- `permute`: explicit reordering of dimensions and memory;\n",
    "- `.contiguous()`: ensure that the tensor is stored in contiguous memory, with no other modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1f8cb14a30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_img = img.permute(1, 2, 0)\n",
    "plt.imshow(new_img.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 2, 5, 3, 6])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See also:\n",
    "z = th.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y = z.t()\n",
    "y.size()\n",
    "# y.view(6)\n",
    "y.contiguous().view(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9481, 1.1642, 0.6245, 1.2485],\n",
      "        [1.1642, 1.5995, 0.7921, 1.7560],\n",
      "        [0.6245, 0.7921, 0.5116, 0.8172],\n",
      "        [1.2485, 1.7560, 0.8172, 2.3189]])\n",
      "tensor([[0.9481, 1.1642, 0.6245, 1.2485],\n",
      "        [1.1642, 1.5995, 0.7921, 1.7560],\n",
      "        [0.6245, 0.7921, 0.5116, 0.8172],\n",
      "        [1.2485, 1.7560, 0.8172, 2.3189]])\n",
      "tensor([[0.9481, 1.1642, 0.6245, 1.2485],\n",
      "        [1.1642, 1.5995, 0.7921, 1.7560],\n",
      "        [0.6245, 0.7921, 0.5116, 0.8172],\n",
      "        [1.2485, 1.7560, 0.8172, 2.3189]])\n"
     ]
    }
   ],
   "source": [
    "x = th.rand(4, 5)\n",
    "y = x.T  # matrix transposition; also .t()\n",
    "\n",
    "print(x @ y)\n",
    "print(x.matmul(y))\n",
    "print(th.matmul(x, y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the operator for matrix multiplication is `@`, not `*`, which indicates the Hadamard (element-wise) product instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3238e-04, 6.3946e-01, 1.2321e-01, 3.0301e-02, 1.5497e-01],\n",
       "        [1.4727e-01, 8.1417e-01, 1.8137e-01, 1.2000e-01, 3.3668e-01],\n",
       "        [8.3307e-03, 2.7935e-01, 1.6567e-01, 5.6478e-02, 1.7778e-03],\n",
       "        [5.4171e-02, 4.5338e-01, 2.9923e-01, 5.4899e-01, 9.6317e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying a matrix by itself is obviously equivalent to computing its power, and it can be done also by running one of the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3238e-04, 6.3946e-01, 1.2321e-01, 3.0301e-02, 1.5497e-01],\n",
      "        [1.4727e-01, 8.1417e-01, 1.8137e-01, 1.2000e-01, 3.3668e-01],\n",
      "        [8.3307e-03, 2.7935e-01, 1.6567e-01, 5.6478e-02, 1.7778e-03],\n",
      "        [5.4171e-02, 4.5338e-01, 2.9923e-01, 5.4899e-01, 9.6317e-01]])\n",
      "tensor([[1.3238e-04, 6.3946e-01, 1.2321e-01, 3.0301e-02, 1.5497e-01],\n",
      "        [1.4727e-01, 8.1417e-01, 1.8137e-01, 1.2000e-01, 3.3668e-01],\n",
      "        [8.3307e-03, 2.7935e-01, 1.6567e-01, 5.6478e-02, 1.7778e-03],\n",
      "        [5.4171e-02, 4.5338e-01, 2.9923e-01, 5.4899e-01, 9.6317e-01]])\n"
     ]
    }
   ],
   "source": [
    "print(th.pow(x, 2))\n",
    "print(x**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in NumPy, there exists a `dot` function to compute the scalar product between vectors. Note that differently from NumPy, in torch this is **not** equivalent to matrix multiplication, as it is intended to work only with 1D vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4]) torch.Size([4])\n",
      "tensor(1.2484)\n",
      "tensor(1.2484)\n",
      "tensor(1.2484)\n"
     ]
    }
   ],
   "source": [
    "v1 = x[:, 1]\n",
    "v2 = x[:, 2]\n",
    "print(v1.shape, v2.shape)\n",
    "\n",
    "print(\n",
    "    v1.dot(v2)\n",
    ")  # in the case of 1D vectors, there is no difference between row and column vectors\n",
    "print(v1.matmul(v2))\n",
    "print(v1 @ v2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do something fancier with two vectors, like multiplying a column by a row to obtain a matrix, you need to switch to 2D vectors by reshaping them.\n",
    "\n",
    "When you reshape a tensor, you can leave one dimension unspecified (using -1), as it can be inferred automatically by th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1]) torch.Size([1, 4])\n",
      "tensor([[0.2807, 0.3406, 0.3255, 0.4374],\n",
      "        [0.3167, 0.3843, 0.3673, 0.4936],\n",
      "        [0.1855, 0.2251, 0.2151, 0.2891],\n",
      "        [0.2364, 0.2868, 0.2741, 0.3683]])\n"
     ]
    }
   ],
   "source": [
    "v1 = v1.reshape(-1, 1)  # column vector\n",
    "v2 = v2.reshape(1, -1)  # row vector\n",
    "\n",
    "print(v1.shape, v2.shape)\n",
    "print(v1 @ v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(v1.dot(v2))    # this doesn't work! dot works only on 1D tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[[0.0868, 0.3113],\n",
      "         [0.7262, 0.8753],\n",
      "         [0.5142, 0.5419]],\n",
      "\n",
      "        [[0.6264, 0.5516],\n",
      "         [0.0195, 0.6374],\n",
      "         [0.8739, 0.4433]]])\n"
     ]
    }
   ],
   "source": [
    "x = th.rand(2, 3, 2)\n",
    "print(f\"X: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.2080), tensor(6.2080))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(), th.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5173), tensor(0.5173))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), th.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8), tensor(8))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.argmin(), th.argmin(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes useful to specify one or more dimensions to reduce (along which you want to perform your operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3566, 0.4314],\n",
       "        [0.3729, 0.7564],\n",
       "        [0.6941, 0.4926]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [2, 1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8472, 3.3609])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(dim=(0, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Linear regression\n",
    "\n",
    "By using all the pieces we've seen till now, we can build our first *model* using PyTorch: a linear regressor, i.e.:\n",
    "\n",
    "$$\n",
    "y = XW + b\n",
    "$$\n",
    "\n",
    "which can also be simplified as:\n",
    "\n",
    "$$\n",
    "y = XW\n",
    "$$\n",
    "\n",
    "if we incorporate the bias $b$ inside $W$ and add to the $X$ a column of ones to the right.\n",
    "\n",
    "\n",
    "We start by generating our data. We randomly sample $X$ as a $N\\times P$ tensor, meaning that we have 1000 datapoints and 100 features and produce $y$ as:\n",
    "$$\n",
    "y=XM+\\mathcal{N}(0,I)\n",
    "$$\n",
    "where $M$ is a randomly drawn projection vector (shape $P\\times 1$, same as our weights).\n",
    "We are adding some iid gaussian noise on the $y$ to avoid the interpolation regime, in which we could be fitting our data perfectly using a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "P = 100\n",
    "X = th.rand(N, P)\n",
    "M = th.rand(P, 1)\n",
    "y = X @ M + th.normal(th.zeros(N, 1), th.ones(N, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a column of ones to $X$ to include the bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = th.cat([X, th.ones(N, 1)], dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression can be fit with classical statistical methods such as Ordinary Least Squares, and the optimal $W$ has the form:\n",
    "\n",
    "$$\n",
    "W^*=(X^TX)^{-1}X^Ty\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_star = ((X.T @ X).inverse()) @ X.T @ y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the quality of this fit we can evaluate the Mean Squared Error (MSE) between the original $y$ and the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9004)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.nn.functional.mse_loss(X @ W_star, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not just `numpy` (1): Automatic differentiation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how ``autograd`` collects gradients. We create two tensors ``a`` and ``b`` with\n",
    "``requires_grad=True``. This signals to ``autograd`` that every operation on them should be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = th.tensor([2.0, 3.0], requires_grad=True)\n",
    "b = th.tensor([6.0, 4.0], requires_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create another tensor ``Q`` from ``a`` and ``b``.\n",
    "\n",
    "\\begin{align}Q = 3a^3 - b^2\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3 * a**3 - b**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the gradients of ``Q`` w.r.t. ``a`` and ``b``, i.e.:\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n",
    "\n",
    "We can do this by calling ``.backward()`` on any **scalar** function of ``Q``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9 * a**2 == a.grad)\n",
    "print(-2 * b == b.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not just `numpy` (2): GPU Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[255.6709, 250.9626, 250.5085,  ..., 250.6259, 247.4780, 260.9814],\n",
      "        [252.5892, 243.0260, 242.4091,  ..., 245.3009, 242.6892, 250.9973],\n",
      "        [256.8336, 250.1258, 244.9961,  ..., 247.6424, 239.4887, 258.1257],\n",
      "        ...,\n",
      "        [254.9937, 239.6047, 242.8213,  ..., 246.0543, 241.6772, 255.3834],\n",
      "        [258.1039, 248.5307, 245.7037,  ..., 250.8511, 247.7471, 252.2103],\n",
      "        [249.8258, 244.6473, 244.7070,  ..., 247.4586, 237.3992, 255.7619]])\n"
     ]
    }
   ],
   "source": [
    "a = th.rand(1000, 1000)\n",
    "b = th.rand(1000, 1000)\n",
    "print(th.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    r = th.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[255.6709, 250.9626, 250.5086,  ..., 250.6259, 247.4780, 260.9814],\n",
      "        [252.5891, 243.0261, 242.4091,  ..., 245.3009, 242.6891, 250.9973],\n",
      "        [256.8337, 250.1258, 244.9962,  ..., 247.6424, 239.4886, 258.1257],\n",
      "        ...,\n",
      "        [254.9937, 239.6048, 242.8213,  ..., 246.0543, 241.6772, 255.3834],\n",
      "        [258.1039, 248.5307, 245.7037,  ..., 250.8510, 247.7470, 252.2103],\n",
      "        [249.8258, 244.6472, 244.7069,  ..., 247.4586, 237.3992, 255.7618]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a_cuda = a.cuda()  # Or, generally: a.to(device)\n",
    "b_cuda = b.cuda()  # Or, generally: a.to(device)\n",
    "print(th.matmul(a_cuda, b_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    r = th.matmul(a_cuda, b_cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
