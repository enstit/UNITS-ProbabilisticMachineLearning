{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2a: *Gaussian Processes* (part 1)\n",
    "\n",
    "Probabilistic Machine Learning -- Spring 2023, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/emaballarin/probml-units/blob/main/notebooks/02a_gaussian_processes_mle.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Gaussian Processes*** (GPs) are *stochastic processes*, *i.e.* collections of random variables, such that every finite collection of such has a multivariate normal distribution. \n",
    "GPs can be interpreted as ***distributions over functions*** $f(x)$. In fact, whenever we observe a finite set of values $(f(x_1),\\ldots,f(x_N))$, we assume they are jointly normally distributed with some mean $\\mu(x)$ and covariance $\\Sigma(x)$. Let\n",
    "\n",
    "$$ f(x) \\sim \\mathcal{GP}\\big(\\mu(x), \\Sigma(x)\\big)$$\n",
    "\n",
    "denote a Gaussian Process, where for any finite subset $\\bar{x}=(x_1,\\ldots,x_N)$, the marginal distribution is a multivariate Gaussian $ f(\\bar{x})\\sim \\mathcal{N}\\big(\\mu(\\bar{x}), \\Sigma(\\bar{x})\\big).$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "The covariance matrix $\\Sigma(x)$ of a GP is defined by a ***kernel function*** $k$. To ensure that the learned function preserves similarity between the inputs, it is required that $ \\Sigma_{ij}=k(x_i,x_j). $\n",
    "\n",
    "A kernel (or covariance function) $k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow \\mathbb{R}$ defines the statistical relationship between two variables in a space $\\mathcal{X}$. It is symmetric and non-negative, meaning that \n",
    "\n",
    "\\begin{align*}\n",
    "k(x,x')&=k(x',x)\\\\\n",
    "k(x,x')&\\geq0\n",
    "\\end{align*}\n",
    "\n",
    "so it acts as a measure of similarity in $\\mathcal{X}$ and the resulting matrix $\\Sigma(x)$ is always symmetric and positive definite.\n",
    "\n",
    "The choice of a specific kernel function allows to set a prior distribution over functions.\n",
    "\n",
    "Examples of valid kernels are *e.g.* the ***radial basis function*** (RBF) kernel\n",
    "\n",
    "$$\n",
    "k(x,x')=\\sigma^2 \\exp \\Bigg(-\\frac{||x-x'||^2}{2 l^2}\\Bigg),\n",
    "$$\n",
    "\n",
    "the (uniform) ***polynomial*** kernel \n",
    "\n",
    "$$\n",
    "k(x,x')=\\sigma^2 (x^T x'+b)^d\n",
    "$$\n",
    "\n",
    "and the **periodic** kernel \n",
    "\n",
    "$$ k(x,x')=\\sigma^2 \\exp\\Bigg( -\\frac{2 \\sin^2 (\\pi (x-x')/p)}{l^2} \\Bigg).\n",
    "$$\n",
    "\n",
    "![](./img/gp_mle_1.png)\n",
    "<br><sub><sup>Example of covariance matrix of kernels computed on 50 equispaced points (left) and 3 samples drawn from a Normal distribution with such covariance and zero mean (right). Kernel parameters: (sigma = 1; l = 1; b = 0.5; p = 3) </a></sup></sub>\n",
    "\n",
    "Kernel functions can also be combined together to generate more complex kernels, by using a set of suitable operations that preserve kernel properties. E.g. we can use products and linear combinations of kernels. \n",
    "\n",
    "![](./img/gp_mle_2.png)\n",
    "<br><sub><sup>Examples of composition of kernels (same parameters as before)</a></sup></sub>\n",
    "\n",
    "\n",
    "## *GP regression*\n",
    "\n",
    "We want to model a dependent variable $y$ as a function of an independent variable $x$, using a set of observations $D=\\{(x_i,y_i)\\}_i$. \n",
    "\n",
    "Classical Bayesian regression fits a parametric function $f(\\theta,\\cdot)$ to the data, i.e. it infers a probability distribution $f(\\theta|D)$. Rather than learning single point estimates of $\\theta$, it places a prior over parameters and updates the distribution whenever new data points are observed.\n",
    "\n",
    "GPs, instead, have a ***non-parametric*** approach that infers a distribution $p(f|D)$ over all possible functions $f$. \n",
    "\n",
    "### Case: Noise-free observations\n",
    "\n",
    "Consider a 1-dimensional GP with mean $\\mu=\\mathbf{0}$ and an RBF kernel with $\\sigma=1$ and $l=0.2$.\n",
    "\n",
    "Given a test set $X_*=x_1,\\ldots,x_N$, we want to predict the outputs $f_*=(f(x_1),\\ldots,f(x_N))$.\n",
    "Before observing any training data, we can sample from the prior distribution on $X_*$\n",
    "\n",
    "$$\n",
    "f_*\\sim\\mathcal{N}(\\mathbf{0},K_{**}),\n",
    "$$\n",
    "\n",
    "where $K_{**}=k(X_*, X_*)$.\n",
    "\n",
    "![](./img/gp_mle_3.png)\n",
    "<br><sub><sup>Plot of 3 samples from the GP prior, defined on the same equispaced points (sigma = 1; l = 0.2)</a></sup></sub>\n",
    "\n",
    "Suppose we observe a training set $X=\\{(x_i, y_i)\\}_i$, where the values of $y_i=f(x_i)$ are exact (i.e. non-noisy). The **joint distribution** on training and test points has the following form\n",
    "\n",
    "$$\n",
    "\\binom{f}{f_*} \\sim \\mathcal{N} \\Bigg(\\binom{\\mu}{\\mu_*} , \n",
    "\\begin{pmatrix} K & K_* \\\\ K_*^T & K_{**} \\end{pmatrix}  \\Bigg),\n",
    "$$\n",
    "\n",
    "where $K=k(X,X)$ and $K_*=k(X,X_*)$.\n",
    "\n",
    "\n",
    "Now we can ***condition* on the training set** $X$ and obtain posterior mean and posterior covariance:\n",
    "\n",
    "\\begin{align*}\n",
    "f_*|X_*,X,f &\\sim \\mathcal{N}(\\mu_*,\\Sigma_*)\\\\\n",
    "\\mu_* &= \\mu(X_*) + K_*^T K^{-1} (f-\\mu(X))\\\\\n",
    "\\Sigma_* &= K_{**}-K_*^T K^{-1} K_*.\n",
    "\\end{align*}\n",
    "\n",
    "![](./img/gp_mle_4.png)\n",
    "<br><sub><sup>Noise-free regime. 3 samples from the GP, after conditioning on some training points (blue dots). The mean (dashed red line) and 1-standard-deviation credibility interval (transparent fill) are also shown.</a></sup></sub>\n",
    "\n",
    "Notice that since the observations are noiseless, the GP acts as an **interpolator** of the training data.\n",
    "\n",
    "### Case: Noisy observations\n",
    "\n",
    "Now suppose that the observations are noisy \n",
    "$$y_i=f(x_i)+\\epsilon \\qquad \\epsilon \\sim\\mathcal{N}(0, \\sigma^2).$$\n",
    "\n",
    "![](./img/gp_mle_5.png)\n",
    "<br><sub><sup>Noisy regime. 3 samples from the GP, after conditioning on some training points (blue dots): the same points as before, with added Gaussian homoskedastic (sigma = 0.5) noise. The mean (dashed red line) and 1-standard-deviation credibility interval (transparent fill) are also shown.</a></sup></sub>\n",
    "\n",
    "In this case, the model cannot interpolate the training points, but only get closer to them after conditing. By changing the kernel hyperparameters we could obtain a significantly different fit to the data.\n",
    "\n",
    "![](./img/gp_mle_6a.png)\n",
    "![](./img/gp_mle_6b.png)\n",
    "<br><sub><sup>Effect of different hyperparameter choices on the learned kernel. Training data is the same as before. Above: sigma = 0.3, l = 0.2; Below: sigma = 1, l = 0.5 </a></sup></sub>\n",
    "\n",
    "### Learning kernel (hyper)parameters\n",
    "\n",
    "Instead of arbitrarily choosing the kernel hyperparameters, we could infer them directly from the observed data. *E.g.* we could optimize the kernel hyperparameters $\\theta$ by maximizing the marginal log-likelihood $ p(y| x,\\theta)$ on the observed data $(x,y)$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\text{argmax}_\\theta p(y| x,\\theta).\n",
    "$$\n",
    "\n",
    "![](./img/gp_mle_7.png)\n",
    "<br><sub><sup>3 samples from the GP, after conditioning on some training points (blue dots): the same points as before, with added Gaussian homoskedastic (sigma = 0.5) noise. The mean (dashed red line) and 1-standard-deviation credibility interval (transparent fill) are also shown. Kernel hyperparameters fitted by MLL of the data.</a></sup></sub>\n",
    "\n",
    "## References:\n",
    "[1] K. P. Murphy, [\"Machine Learning: A Probabilistic Perspective\"](http://noiselab.ucsd.edu/ECE228/Murphy_Machine_Learning.pdf) (Chap. 15)\n",
    "\n",
    "[2] C. M. Bishop, [\"Pattern Recognition and Machine Learning\"](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) (Chap. 6)\n",
    "\n",
    "[3] [SKlearn GPs documentation](https://scikit-learn.org/stable/modules/gaussian_process.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
